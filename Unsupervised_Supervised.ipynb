{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised_Supervised.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFZESrMJ9TVxaUwfF2zJtC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purendra25/ds/blob/master/Unsupervised_Supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzOXgtEfiA-O"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Oct  5 04:05:02 2020\n",
        "\n",
        "@author: tjha\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"SOM.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1Qd72k2xntZ0F49e8zn4ox9IOZcnWwTBk\n",
        "\n",
        "#Self Organizing Map\n",
        "\n",
        "##Install MiniSom Package\n",
        "\"\"\"\n",
        "\n",
        "#nda install minisom\n",
        "\n",
        "\n",
        "\"\"\"### Importing the libraries\"\"\"\n",
        "\n",
        "\n",
        "#conda install numpy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"## Importing the dataset\"\"\"\n",
        "\n",
        "dataset = pd.read_csv('Credit_Card_Applications.csv')\n",
        "X = dataset.iloc[:, :-1].values \n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "\"\"\"## Feature Scaling\"\"\"\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler(feature_range = (0,1))\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "\"\"\"##Training the SOM\"\"\"\n",
        "\"\"\"\n",
        "from minisom import MiniSom\n",
        "som = MiniSom(x=10, y=10, input_len= 15, sigma= 1.0, learning_rate = 0.5)\n",
        "som.random_weights_init(X)\n",
        "som.train_random(data = X, num_iteration = 100)\n",
        "\n",
        "##Visualizing the results\n",
        "\n",
        "from pylab import bone, pcolor, colorbar, plot, show\n",
        "bone()\n",
        "pcolor(som.distance_map().T)\n",
        "colorbar()\n",
        "markers = ['o', 's']\n",
        "colors = ['r', 'g']\n",
        "for i, x in enumerate(X):\n",
        "    w = som.winner(x)\n",
        "    plot(w[0] + 0.5,\n",
        "         w[1] + 0.5,\n",
        "         markers[y[i]],\n",
        "         markeredgecolor = colors[y[i]],\n",
        "         markerfacecolor = 'None',\n",
        "         markersize = 10,\n",
        "         markeredgewidth = 2)\n",
        "show()\n",
        "\n",
        "#\"\"\"\n",
        "## Finding the frauds\n",
        "\n",
        "mappings = som.win_map(X)\n",
        "frauds = np.concatenate((mappings[(5,4)], mappings[(6,2)]), axis = 0)\n",
        "#frauds = mappings[(1,8)]\n",
        "frauds = sc.inverse_transform(frauds)\n",
        "\n",
        "# creating matrix of feature\n",
        "\n",
        "\n",
        "customers = dataset.iloc[:,1:].values\n",
        "\n",
        "is_fraud = np.zeros(len(dataset))\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    if dataset.iloc[i,0] in frauds:\n",
        "       is_fraud[i]=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "customers = sc.fit_transform(customers)\n",
        "\n",
        "# Part 2 - Building the ANN\n",
        "\n",
        "# Initializing the ANN\n",
        "ann = tf.keras.models.Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "ann.add(tf.keras.layers.Dense(units=2, activation='relu',input_dim=15))\n",
        "\n",
        "\n",
        "# Adding the output layer\n",
        "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Part 3 - Training the ANN\n",
        "\n",
        "# Compiling the ANN\n",
        "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Training the ANN on the Training set\n",
        "#ann.fit(customers, is_fraud, batch_size = 1, epochs = 2)\n",
        "\n",
        "# Part 4 - Making the predictions and evaluating the model\n",
        "\n",
        "# Predicting the result of a single observation\n",
        "\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = ann.predict(customers)\n",
        "\n",
        "#print(dataset.iloc[:,0:1].values)\n",
        "\n",
        "y_pred = np.concatenate((dataset.iloc[:,0:1].values,y_pred),axis=1)\n",
        "\n",
        "y_pred = y_pred[y_pred[:,1].argsort()]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}